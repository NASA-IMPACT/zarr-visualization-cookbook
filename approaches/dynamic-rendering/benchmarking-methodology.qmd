---
title: Benchmarking Methodology
---

## End-to-End Benchmarks

The end-to-end benchmarks capture the user experience for various interactions with a deployment of the dynamic rendering approach. The suite of benchmarks included in this cookbook are designed to inform how the choice of Zarr versions and chunking schemes influence the user experience.

### End-to-End Benchmarks: Datasets

We used the publicly available [NASA Earth Exchange Global Daily Downscaled Projections (NEX-GDDP-CMIP6)](https://aws.amazon.com/marketplace/pp/prodview-k6adk576fiwmm#overview) AWS public dataset for this project. For this demonstration, we used two years of the the daily maximum near-surface air temperature (tasmax) variable from the `ACCESS-CM2` climage model.

We first leveraged the [pangeo-forge-recipes](https://pangeo-forge.readthedocs.io/en/latest/pangeo_forge_recipes/index.html) Python package to transform the NetCDF files hosted on S3 to Zarr stores, with the full notebook available in the [benchmark-maps repository](https://github.com/carbonplan/benchmark-maps/blob/main/stores/01_cmip6_netcdf_to_zarr.ipynb).


Next, we leveraged [ndpyramid](https://github.com/carbonplan/ndpyramid) to generate pyramids for the Zarr store. The notebook for generating pyramids is available in the [benchmark-maps code repository](https://github.com/carbonplan/benchmark-maps/blob/main/stores/02_zarr_to_pyramids.ipynb). 

### End-to-End Benchmarks: Approach

CarbonPlan's [benchmark-maps](https://github.com/carbonplan/benchmark-maps) repository leverages [Playwright](https://playwright.dev/python/) for the end-to-end performance benchmarks. By default, the benchmarks are run on [https://prototype-maps.vercel.app/](https://prototype-maps.vercel.app/) although the url is configurable. The frame below shows this domain after selecting an approach, Zarr version, and dataset.

The benchmarking script takes the following steps:

1. Launch chromium browser 
2. Create a new page
3. Start chromium tracing
4. Select Zarr version in the dropdown
5. Select Dataset in the dropbown
6. Wait 5 seconds for the page the render
7. Zoom in a defined number of times, waiting 5 seconds after each action
8. Write out metadata about each run and the trace record

The frame rate and request information are extracted from the resultant metadata and trace records. The completion time for each zoom level is determined by comparing the screen captures in the trace record to the expected result for each zoom level.
