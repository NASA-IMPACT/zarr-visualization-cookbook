[
  {
    "objectID": "approaches/tiling.html",
    "href": "approaches/tiling.html",
    "title": "Tiling",
    "section": "",
    "text": "The tiling approach is to provide image tiles via the XYZ Protocol and OGC API - Tiles API specifications.\nThis approach relies on the rio_tiler.XarrayReader library which includes the tile function. This function and others in that module are used to provide an API for tiles. An example API infrastructure can be found in titiler-xarray. Please note this library is still in development and is not intended for production use at this time."
  },
  {
    "objectID": "approaches/index.html",
    "href": "approaches/index.html",
    "title": "Approaches",
    "section": "",
    "text": "For browser-based visualization of Zarr, there are 2 approaches covered in this cookbook:\n\nTiling\nDirect Client\n\nThe tile server provides an API which is interoperable with multiple interfaces, but requires maintaining a tile server. also the response delivered to the client is an image format, not the raw data itself. The direct client has access to the underlying data and thus maximum flexibility in rendering and analysis for the user."
  },
  {
    "objectID": "approaches/direct-client/e2e-results.html",
    "href": "approaches/direct-client/e2e-results.html",
    "title": "End-to-End Benchmarking",
    "section": "",
    "text": "The CarbonPlan team put together some utilities for parsing, processing, and visualizing the benchmarking results in carbonplan_benchmarks. We’ll use those utilities along with the Holoviz HoloViz suite of tools for visualization and Pandas as the underlying analysis tool.\n\nimport carbonplan_benchmarks.analysis as cba\nimport hvplot\nimport pandas as pd\n\npd.options.plotting.backend = \"holoviews\"\n\n\n\n\nFirst, define the paths to the baseline images that the tests will be compared against and paths to the metadata files associated with each benchmarking run.\n\nbaseline_fp = \"s3://carbonplan-benchmarks/benchmark-data/baselines.json\"\nmetadata_base_fp = \"s3://carbonplan-benchmarks/benchmark-data\"\nmetadata_files = [\n    \"data-2023-08-04T01-14-24.json\",\n    \"data-2023-08-04T01-15-30.json\",\n    \"data-2023-08-04T01-16-27.json\",\n    \"data-2023-08-04T01-17-25.json\",\n    \"data-2023-08-04T01-18-37.json\",\n    \"data-2023-08-04T01-19-47.json\",\n    \"data-2023-08-04T01-21-02.json\",\n    \"data-2023-08-04T01-22-08.json\",\n]\n\nNow, use the utilities from carbonplan_benchmarks to load the metadata and baseline images into DataFrames, process those results, and create a summary DataFrame for all runs.\n\nsnapshots = cba.load_snapshots(snapshot_path=baseline_fp)\nsummary_dfs = []\nfor file in metadata_files:\n    fp = f\"{metadata_base_fp}/{file}\"\n    metadata, trace_events = cba.load_data(metadata_path=fp, run=0)\n    data = cba.process_run(\n        metadata=metadata, trace_events=trace_events, snapshots=snapshots\n    )\n    summary_dfs.append(cba.create_summary(metadata=metadata, data=data))\nsummary = pd.concat(summary_dfs)\n\n\nsummary.head(n=8)\n\n\n\n\n\n\n\n\naction\nbrowser_name\nbrowser_version\nplaywright_python_version\nprovider\ntrace_path\nurl\nzoom_level\napproach\nzarr_version\ndataset\nchunk_size\nzoom\nduration\nfps\nrequest_duration\nrequest_percent\n\n\n\n\n0\nzoom_in\nchromium\n115.0.5790.75\n1.36.0\nunknown\ns3://carbonplan-benchmarks/benchmark-data/2023...\nhttps://prototype-maps.vercel.app/direct-clien...\n4\ndirect-client\nv2\n1MB-chunks\n1\n0.0\n1039.890\n50.005289\n962.687\n92.575849\n\n\n1\nzoom_in\nchromium\n115.0.5790.75\n1.36.0\nunknown\ns3://carbonplan-benchmarks/benchmark-data/2023...\nhttps://prototype-maps.vercel.app/direct-clien...\n4\ndirect-client\nv2\n1MB-chunks\n1\n1.0\n1076.482\n54.808162\n738.299\n68.584426\n\n\n2\nzoom_in\nchromium\n115.0.5790.75\n1.36.0\nunknown\ns3://carbonplan-benchmarks/benchmark-data/2023...\nhttps://prototype-maps.vercel.app/direct-clien...\n4\ndirect-client\nv2\n1MB-chunks\n1\n2.0\n702.452\n59.790562\n361.903\n51.519962\n\n\n3\nzoom_in\nchromium\n115.0.5790.75\n1.36.0\nunknown\ns3://carbonplan-benchmarks/benchmark-data/2023...\nhttps://prototype-maps.vercel.app/direct-clien...\n4\ndirect-client\nv2\n1MB-chunks\n1\n3.0\n981.422\n39.738257\n646.134\n65.836511\n\n\n4\nzoom_in\nchromium\n115.0.5790.75\n1.36.0\nunknown\ns3://carbonplan-benchmarks/benchmark-data/2023...\nhttps://prototype-maps.vercel.app/direct-clien...\n4\ndirect-client\nv2\n1MB-chunks\n1\n4.0\n475.924\n60.934099\n0.000\n0.000000\n\n\n0\nzoom_in\nchromium\n115.0.5790.75\n1.36.0\nunknown\ns3://carbonplan-benchmarks/benchmark-data/2023...\nhttps://prototype-maps.vercel.app/direct-clien...\n4\ndirect-client\nv2\n5MB-chunks\n5\n0.0\n1199.615\n34.177632\n1107.553\n92.325704\n\n\n1\nzoom_in\nchromium\n115.0.5790.75\n1.36.0\nunknown\ns3://carbonplan-benchmarks/benchmark-data/2023...\nhttps://prototype-maps.vercel.app/direct-clien...\n4\ndirect-client\nv2\n5MB-chunks\n5\n1.0\n1362.769\n53.567406\n996.513\n73.124132\n\n\n2\nzoom_in\nchromium\n115.0.5790.75\n1.36.0\nunknown\ns3://carbonplan-benchmarks/benchmark-data/2023...\nhttps://prototype-maps.vercel.app/direct-clien...\n4\ndirect-client\nv2\n5MB-chunks\n5\n2.0\n2902.872\n25.147509\n2558.564\n88.139057"
  },
  {
    "objectID": "approaches/direct-client/e2e-results.html#processing-benchmark-results",
    "href": "approaches/direct-client/e2e-results.html#processing-benchmark-results",
    "title": "End-to-End Benchmarking",
    "section": "",
    "text": "The CarbonPlan team put together some utilities for parsing, processing, and visualizing the benchmarking results in carbonplan_benchmarks. We’ll use those utilities along with the Holoviz HoloViz suite of tools for visualization and Pandas as the underlying analysis tool.\n\nimport carbonplan_benchmarks.analysis as cba\nimport hvplot\nimport pandas as pd\n\npd.options.plotting.backend = \"holoviews\"\n\n\n\n\nFirst, define the paths to the baseline images that the tests will be compared against and paths to the metadata files associated with each benchmarking run.\n\nbaseline_fp = \"s3://carbonplan-benchmarks/benchmark-data/baselines.json\"\nmetadata_base_fp = \"s3://carbonplan-benchmarks/benchmark-data\"\nmetadata_files = [\n    \"data-2023-08-04T01-14-24.json\",\n    \"data-2023-08-04T01-15-30.json\",\n    \"data-2023-08-04T01-16-27.json\",\n    \"data-2023-08-04T01-17-25.json\",\n    \"data-2023-08-04T01-18-37.json\",\n    \"data-2023-08-04T01-19-47.json\",\n    \"data-2023-08-04T01-21-02.json\",\n    \"data-2023-08-04T01-22-08.json\",\n]\n\nNow, use the utilities from carbonplan_benchmarks to load the metadata and baseline images into DataFrames, process those results, and create a summary DataFrame for all runs.\n\nsnapshots = cba.load_snapshots(snapshot_path=baseline_fp)\nsummary_dfs = []\nfor file in metadata_files:\n    fp = f\"{metadata_base_fp}/{file}\"\n    metadata, trace_events = cba.load_data(metadata_path=fp, run=0)\n    data = cba.process_run(\n        metadata=metadata, trace_events=trace_events, snapshots=snapshots\n    )\n    summary_dfs.append(cba.create_summary(metadata=metadata, data=data))\nsummary = pd.concat(summary_dfs)\n\n\nsummary.head(n=8)\n\n\n\n\n\n\n\n\naction\nbrowser_name\nbrowser_version\nplaywright_python_version\nprovider\ntrace_path\nurl\nzoom_level\napproach\nzarr_version\ndataset\nchunk_size\nzoom\nduration\nfps\nrequest_duration\nrequest_percent\n\n\n\n\n0\nzoom_in\nchromium\n115.0.5790.75\n1.36.0\nunknown\ns3://carbonplan-benchmarks/benchmark-data/2023...\nhttps://prototype-maps.vercel.app/direct-clien...\n4\ndirect-client\nv2\n1MB-chunks\n1\n0.0\n1039.890\n50.005289\n962.687\n92.575849\n\n\n1\nzoom_in\nchromium\n115.0.5790.75\n1.36.0\nunknown\ns3://carbonplan-benchmarks/benchmark-data/2023...\nhttps://prototype-maps.vercel.app/direct-clien...\n4\ndirect-client\nv2\n1MB-chunks\n1\n1.0\n1076.482\n54.808162\n738.299\n68.584426\n\n\n2\nzoom_in\nchromium\n115.0.5790.75\n1.36.0\nunknown\ns3://carbonplan-benchmarks/benchmark-data/2023...\nhttps://prototype-maps.vercel.app/direct-clien...\n4\ndirect-client\nv2\n1MB-chunks\n1\n2.0\n702.452\n59.790562\n361.903\n51.519962\n\n\n3\nzoom_in\nchromium\n115.0.5790.75\n1.36.0\nunknown\ns3://carbonplan-benchmarks/benchmark-data/2023...\nhttps://prototype-maps.vercel.app/direct-clien...\n4\ndirect-client\nv2\n1MB-chunks\n1\n3.0\n981.422\n39.738257\n646.134\n65.836511\n\n\n4\nzoom_in\nchromium\n115.0.5790.75\n1.36.0\nunknown\ns3://carbonplan-benchmarks/benchmark-data/2023...\nhttps://prototype-maps.vercel.app/direct-clien...\n4\ndirect-client\nv2\n1MB-chunks\n1\n4.0\n475.924\n60.934099\n0.000\n0.000000\n\n\n0\nzoom_in\nchromium\n115.0.5790.75\n1.36.0\nunknown\ns3://carbonplan-benchmarks/benchmark-data/2023...\nhttps://prototype-maps.vercel.app/direct-clien...\n4\ndirect-client\nv2\n5MB-chunks\n5\n0.0\n1199.615\n34.177632\n1107.553\n92.325704\n\n\n1\nzoom_in\nchromium\n115.0.5790.75\n1.36.0\nunknown\ns3://carbonplan-benchmarks/benchmark-data/2023...\nhttps://prototype-maps.vercel.app/direct-clien...\n4\ndirect-client\nv2\n5MB-chunks\n5\n1.0\n1362.769\n53.567406\n996.513\n73.124132\n\n\n2\nzoom_in\nchromium\n115.0.5790.75\n1.36.0\nunknown\ns3://carbonplan-benchmarks/benchmark-data/2023...\nhttps://prototype-maps.vercel.app/direct-clien...\n4\ndirect-client\nv2\n5MB-chunks\n5\n2.0\n2902.872\n25.147509\n2558.564\n88.139057"
  },
  {
    "objectID": "approaches/direct-client/e2e-results.html#visualize-results",
    "href": "approaches/direct-client/e2e-results.html#visualize-results",
    "title": "End-to-End Benchmarking",
    "section": "Visualize results",
    "text": "Visualize results\nFirst, let’s see how the duration of each action changes as a function of the zoom level. An important piece of context is that the underlying dataset only has four pyramid levels, so zoom=4 does not need to fetch any new data.\n\nsummary.plot.scatter(x=\"zoom\", y=\"duration\", by=\"zarr_version\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nNow, let’s instead show the duration as a function of the chunk size.\n\nsummary.plot.scatter(x=\"chunk_size\", y=\"duration\", by=\"zarr_version\")\n\n\n\n\n\n  \n\n\n\n\nNow, let’s look at the request duration as a funciton of the chunk size.\n\nsummary.plot.scatter(x=\"chunk_size\", y=\"request_duration\", by=\"zarr_version\")\n\n\n\n\n\n  \n\n\n\n\nLastly, let’s look at the fraction of time that’s spent fetching data as a function of the chunk size.\n\nsummary.plot.scatter(x=\"chunk_size\", y=\"request_percent\", by=\"zarr_version\").opts(\n    ylim=(0, 100)\n)"
  },
  {
    "objectID": "approaches/tiling/cog-pgstac.html",
    "href": "approaches/tiling/cog-pgstac.html",
    "title": "Profiling tiling code for pgSTAC + COG",
    "section": "",
    "text": "A pgSTAC database stores metadata about CMIP6 COGs on S3. The libraries used were pgstac for reading STAC metadata and rio_tiler’s rasterio for reading COGs on S3.\nIn this notebook we load results from https://github.com/developmentseed/tile-benchmarking/blob/main/profiling/profile.ipynb to demonstrate:\n\nThe importance of GDAL variables in performance.\nVariation across tiles is not significant.\nTiling with pgSTAC + COGs is fast when compared with titiler-xarray tiling of Zarr stores.\n\n\nimport pandas as pd\nimport hvplot\npd.options.plotting.backend = 'holoviews'\n\ngit_url_path = \"https://raw.githubusercontent.com/developmentseed/tile-benchmarking/feat/fake-data/profiling/results\"\npd.read_csv(f\"{git_url_path}/pgstac_cog_gdal_results.csv\")\n\n\n\n\n\n\n\n\ngdal_vars_set?\ntile times\nmean total time\n\n\n\n\n0\nwith_gdal_vars\n[63.41, 54.53, 54.46]\n57.466667\n\n\n1\nwithout_gdal_vars\n[14687.78, 30817.34, 17722.72]\n21075.946667\n\n\n\n\n\n\n\nWe don’t need many iterations since the variation is so great. You can see that setting GDAL environment variables makes things at least 100x faster.\nThese GDAL variables are documented here https://developmentseed.org/titiler/advanced/performance_tuning/, but that advice is copied into comments below for ease of reference.\nBy setting the GDAL environment variables we limit the number of total requests to S3.\nSpecifically, these environment variables ensure that:\n\nAll of the metadata may be read in 1 request. This is not necessarily true, but more likely since we increase the initial number of GDAL ingested bytes.\nThere is no superfluous LIST request to account for sidecar files, which don’t exist for COGs.\nConsecutive range requests are merged into 1 request.\nMultiple range requests use the same TCP connection.\n\n\nTime to create different tiles\nThe difference between different tiles is very small.\n\ndf = pd.read_csv(f\"{git_url_path}/pgstac_cog_tile_results.csv\")\ndf.plot.scatter(x='xyz tile', y='mean total time', label = 'Mean Time to Tile (ms) by Zoom Level')"
  },
  {
    "objectID": "approaches/tiling/benchmarking-methodology.html",
    "href": "approaches/tiling/benchmarking-methodology.html",
    "title": "Benchmarking Methodology",
    "section": "",
    "text": "While end-to-end tests are useful to understand tiling performance as experienced by an end user, they are not useful to understanding the performance of the code used to create image tiles. End-to-end tests include the network of the requesting client and the responding server, which can be highly variable and difficult to control. To understand the performance of the underlying libraries, we used code profiling.\n\n\nReproducibility is important to the integrity of this project and its reported results. That is why we selected a publicly available dataset and attempt to make the steps as fully documented and reproducible as possible. For code profiling, we focused the NASA Earth Exchange Global Daily Downscaled Projections (NEX-GDDP-CMIP6) AWS public dataset for this project.\nThere are 2 datasets listed on AWS: 1 is an archive of NetCDF files from about 35 different climate models, each supplying historical and predicted values for up to 9 environment variables, daily, from 1950 to 2100. To minimize preprocessing, test datasets were generated for the first 2 years of historical data, for 1 model and for 1 variable. These variables or dataset could easily be modified or swapped, but we expect the relative performance using different datasets to be the same.\nIn addition to the NetCDF data, there is an archives of COGs generated from that NetCDF data to support visualization via dynamic tiling using COGs. COGs are only available for 2 models, so for inter comparison of the tiling approach between COGs and Zarr, one of those models (GISS-E2-1-G) is used to generate Zarr stores.\nTODO: At this time, a different model is used for the direct client benchmarks (ACCESS-CM2), but we plan to demonstrate how there is no meaningful difference in the performance of tiling across these models.\nCode profiling for the tiling approach were run on the VEDA JupyterHub. The VEDA documentation details how to request access: https://nasa-impact.github.io/veda-docs/services/jupyterhub.html. Because we don’t want to make the database or S3 bucket with datasets fully public, you must be logged into the VEDA JupyterHub to run those benchmarks.\n\n\n\nWe include performance results of code for tiling both COGs and Zarr to help data providers decide which format better suits their overall needs. To make these results comparable, we assume 2 step process for creating image tiles. Assuming a collection of interest has been selected and some query parameters such as bounding box, temporal extent, and variable, the first step is to query the metadata for the relevant data files and the second step is to generate tiles from those data files. Details on each step are provided below.\n\nAssume you are starting with a known collection and query parameters\nRead metadata\n\nFor COGs, the query is registered with pgSTAC for the collection id and query parameters, such as variable and datetime.\nFor Zarr, the metadata is “lazily loaded” for the variable and temporal extent from the known collection store.\n\nGenerate tiles\n\nFor COGs, the mosaic ID returned from the registered query is used to read chunks from COGs on S3.\nFor Zarr, the metadata is used by xarray to read chunks from NetCDFs or Zarrs on S3 and the XarrayReader of rio_tiler is used to generate tiles.\n\n\nTo make as close to an apples to apples comparison as possible, we have stored COG metadata using pgSTAC for data in the nex-gddp-cmip6-cog bucket in AWS Relational Database Service (RDS) and Zarr metadata and data files in S3.\nTo profile the code for rendering tiles with both XarrayReader and titiler-pgstac, code was copied from those projects as needed to inject timers and use the cprofile library. Logs to s3fs were also inspected. Results can be reviewed in the Results Summary and its child pages."
  },
  {
    "objectID": "approaches/tiling/benchmarking-methodology.html#code-benchmarks",
    "href": "approaches/tiling/benchmarking-methodology.html#code-benchmarks",
    "title": "Benchmarking Methodology",
    "section": "",
    "text": "While end-to-end tests are useful to understand tiling performance as experienced by an end user, they are not useful to understanding the performance of the code used to create image tiles. End-to-end tests include the network of the requesting client and the responding server, which can be highly variable and difficult to control. To understand the performance of the underlying libraries, we used code profiling.\n\n\nReproducibility is important to the integrity of this project and its reported results. That is why we selected a publicly available dataset and attempt to make the steps as fully documented and reproducible as possible. For code profiling, we focused the NASA Earth Exchange Global Daily Downscaled Projections (NEX-GDDP-CMIP6) AWS public dataset for this project.\nThere are 2 datasets listed on AWS: 1 is an archive of NetCDF files from about 35 different climate models, each supplying historical and predicted values for up to 9 environment variables, daily, from 1950 to 2100. To minimize preprocessing, test datasets were generated for the first 2 years of historical data, for 1 model and for 1 variable. These variables or dataset could easily be modified or swapped, but we expect the relative performance using different datasets to be the same.\nIn addition to the NetCDF data, there is an archives of COGs generated from that NetCDF data to support visualization via dynamic tiling using COGs. COGs are only available for 2 models, so for inter comparison of the tiling approach between COGs and Zarr, one of those models (GISS-E2-1-G) is used to generate Zarr stores.\nTODO: At this time, a different model is used for the direct client benchmarks (ACCESS-CM2), but we plan to demonstrate how there is no meaningful difference in the performance of tiling across these models.\nCode profiling for the tiling approach were run on the VEDA JupyterHub. The VEDA documentation details how to request access: https://nasa-impact.github.io/veda-docs/services/jupyterhub.html. Because we don’t want to make the database or S3 bucket with datasets fully public, you must be logged into the VEDA JupyterHub to run those benchmarks.\n\n\n\nWe include performance results of code for tiling both COGs and Zarr to help data providers decide which format better suits their overall needs. To make these results comparable, we assume 2 step process for creating image tiles. Assuming a collection of interest has been selected and some query parameters such as bounding box, temporal extent, and variable, the first step is to query the metadata for the relevant data files and the second step is to generate tiles from those data files. Details on each step are provided below.\n\nAssume you are starting with a known collection and query parameters\nRead metadata\n\nFor COGs, the query is registered with pgSTAC for the collection id and query parameters, such as variable and datetime.\nFor Zarr, the metadata is “lazily loaded” for the variable and temporal extent from the known collection store.\n\nGenerate tiles\n\nFor COGs, the mosaic ID returned from the registered query is used to read chunks from COGs on S3.\nFor Zarr, the metadata is used by xarray to read chunks from NetCDFs or Zarrs on S3 and the XarrayReader of rio_tiler is used to generate tiles.\n\n\nTo make as close to an apples to apples comparison as possible, we have stored COG metadata using pgSTAC for data in the nex-gddp-cmip6-cog bucket in AWS Relational Database Service (RDS) and Zarr metadata and data files in S3.\nTo profile the code for rendering tiles with both XarrayReader and titiler-pgstac, code was copied from those projects as needed to inject timers and use the cprofile library. Logs to s3fs were also inspected. Results can be reviewed in the Results Summary and its child pages."
  },
  {
    "objectID": "approaches/tiling/benchmarking-methodology.html#end-to-end-benchmarks",
    "href": "approaches/tiling/benchmarking-methodology.html#end-to-end-benchmarks",
    "title": "Benchmarking Methodology",
    "section": "End-to-End Benchmarks",
    "text": "End-to-End Benchmarks\nEnd-to-end tests provide benchmarks of response times for various tiles and datasets to titiler-xarray.\nDetails and code to generate the benchmarks and store the results on S3 is documented in tile-benchmarking/e2e/e2e.ipynb.\n\nEnd-to-End Benchmarks: Datasets\nA variety of datasets was selected for end-to-end testing and hopefully the framework makes it easy to modify and test new datasets and use cases come up. See the e2e-results.ipynb for specific datasets.\n\n\nEnd-to-End Benchmarks: Approach\nCode from https://github.com/bdon/TileSiege was used to generate a set of tile URLs for the selected test datasets. The testing tool https://locust.io/ was used to run tests. Results are stored uploated as CSV files in S3. These results are read and plotted in e2e-results.ipynb."
  },
  {
    "objectID": "approaches/tiling/results-summary.html",
    "href": "approaches/tiling/results-summary.html",
    "title": "Results Summary",
    "section": "",
    "text": "Results Summary\nCOMING SOON."
  },
  {
    "objectID": "transition-from-rmarkdown.html",
    "href": "transition-from-rmarkdown.html",
    "title": "Transition from RMarkdown",
    "section": "",
    "text": "You may already have workflows in RMarkdown and are interested in transitioning to Quarto. There’s no hurry to migrate to Quarto. Keep using Rmarkdown and when you’re ready the migration will be fine.\nHere are some notes as we migrate RMarkdown sites and books.\nTODO: translating R code chunks"
  },
  {
    "objectID": "transition-from-rmarkdown.html#bookdown-to-quarto",
    "href": "transition-from-rmarkdown.html#bookdown-to-quarto",
    "title": "Transition from RMarkdown",
    "section": "Bookdown to Quarto",
    "text": "Bookdown to Quarto\nConverting a Bookdown book to Quarto is slightly more involved than converting a website. A book has chapters whose order must be defined, and likely has citations and cross-refs. Still, conversion is not that hard.\nWe got some practice converting from Bookdown to Quarto by helping Gavin Fay convert his lab’s fantastic onboarding documentation, the Faylab Lab Manual. Here’s the GitHub view before and after.\nOur best first reference material for this was Nick Tierney’s Notes on Changing from Rmarkdown/Bookdown to Quarto. Nick shares some scripts in that post to automate some changes. In our case, the book was small enough that we made all changes manually. Quarto documentation was indispensable.\n\nExperimenting in a low-risk environment\nWe forked a copy of the Faylab Lab manual to the Openscapes organization, and worked in a branch so we could make changes relatively risk-free. We could always fork a new copy of the original if we “broke” something. (Caution: the default when making a pull request from a fork is to push changes to the original upstream repo, not your fork and it does this without warning if you have write-access to that repo.) With local previews it’s easy to test / play with settings to see what they do. We tended to make a change, Preview, then compare the look and functionality of the book to the original. It was helpful to comment out some elements of the configuration file _output.yml after their counterparts had been added to the Quarto configuration file _quarto.yml, or to confirm they were no longer needed, before making the drastic move of deleting them.\n\n\nThe conversion\nHere are the main steps to convert the Faylab Lab manual from Bookdown to Quarto.\nCreate new empty file called _quarto.yml and add book metadata there. The screenshots below\nSet project type as book.\nMove metadata out of index.qmd and into _quarto.yml. Title, author, and publication date were in index.qmd with date set using date: \"Last updated:r Sys.Date()\". Now these are in _quarto.yml with date set using date: last-modified. Note that having R code would require you to adjust code chunk options in the Quarto style (#|). This tripped us up a bit; see GitHub Actions.\nMove chapters listing out of _bookdown.yml and into _quarto.yml.\nAdd page footer to _quarto.yml.\nHere’s what ours looked like when we finished the steps above (_quarto.yml).\n\n\n\n\n\n\n_quarto.yml contents\n\n\n\n\n\n\n\nFaylab Lab Manual\n\n\n\n\n\nChange insertion of images from html style to Quarto style. (Note Quarto calls them “figures”, not “images”.) The following snippet will insert the GitHub octocat logo in a page:\n![](https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png){fig-align=\"left\" width=\"35px\"}\nChange all filename extensions .Rmd -&gt; .qmd (you could Preview after this change and see that the book looks the same). Note that Quarto works with .Rmd files just as well as it does .qmd, so this change is not urgent. In fact, if you have a lot of R code in your .Rmds (unlike the Faylab Lab Manual), there will be additional tinkering needed to make the code chunks happy.\n\n\nCitations\nThe Faylab Lab Manual cited two papers, presenting us with an opportunity to see how easy it is to add references to a Quarto book. Briefly, in the Visual Editor, Insert &gt; Citation &gt; DOI. Pasting the DOI or its full URL, we can insert the citation. This automatically creates a references.bib file and adds the full citations at the bottom of the chapter page (watch demo). In July 2022, we had to manually add a ## References heading, but this may not be necessary in future Quarto updates.\n\n\n\n\n\n\nInsert citation via its DOI using RStudio Visual Editor\n\n\n\n\n\n\n\n\n\n\nPublishing notes\nIf the book’s output is strictly html, there’s no need to specify output-dir in _quarto.yml. The output directory default is _book/, which is what we’d like. If we wanted other types of output like like PDF or EPUB, etc. those single file outputs are also written to the output-dir (Quarto docs).\nIf you currently have a docs/ folder, delete it.\nUpdate .gitignore to ignore _book/. At the same time, we have it ignore caches and a .quarto file:\n/.quarto/\n*_cache/\n_book/\nOnce all is settled, delete _output.yml.\nOnce the Openscapes fork was fully reviewed, we made a pull request from that to the main branch of the book’s repo. Once that was merged, we set up GitHub Actions to render the book. (TODO: instructions for GitHub Actions)\n\n\nGitHub Actions\nThis book was mostly prose and screenshots without any R code. This made the conversion from RMarkdown to Quarto likely more straightforward than if you also needed to adjust code chunk options in the quarto style (#|). Our initial GitHub Action to render the converted Faylab Lab Manual failed because we had a piece of R code - even though the code was commented out! This was resolved when we deleted the line."
  },
  {
    "objectID": "transition-from-rmarkdown.html#distill-to-quarto",
    "href": "transition-from-rmarkdown.html#distill-to-quarto",
    "title": "Transition from RMarkdown",
    "section": "Distill to quarto",
    "text": "Distill to quarto\nWe transitioned our events site from distill to quarto in May 2022 (github view before and after). We followed excellent notes and examples from Nick Tierney and Danielle Navarro.\nAfter we had changed all the files, the Build tab in the RStudio IDE still showed “Build website” rather then “Render Website” and “Preview Website”, and would error when we pushed them (because that button was expecting a distill site, not a quarto site). To fix this, we updated the .Rproj file. Clicking on the .Rproj file in the RStudio IDE will open a dialog box where you can click things you want (you can also open these in a text editor or from the GitHub website to see the actual text). To fix this situation with the Build tab: Project Options &gt; Build Tools &gt; Project Build Tools &gt; None.\nLooking at files /posts/_metadata.yml and _quarto.yml helps see where things are defined. For example, to make event post citations appear, we added citation: true to /posts/_metadata.yml and in _quarto.yml under the website key we set site-url: https://openscapes.github.io/events. We deleted footer.html used with distill because footer is now defined in quarto.yml.\n\nPublishing notes\n\nBackground: Our distill site had been set up to output to a docs folder, and had GitHub Settings &gt; Pages set to look there rather gh-pages branch. (Julie note: this was a new-to-me capability when we set up the events distill site in Spring 2021 so I had forgotten that was an option). We’ve inititally kept this same set-up for now with our events page in _quarto.yml: output-dir: docs. However, this is sub-optimal; better to not have to commit and push these files but to instead have a GitHub Action generate them upon a commit. So the following is what we did -\n\nDon’t specify output-dir in _quarto.yml. The output directory default is _site/, which is what we’d like.\nIf you currently have a docs/ folder (like we did as we were experimenting), delete it.\nUpdate .gitignore to ignore _site/. At the same time, we have it ignore caches and a .quarto file:\n/.quarto/\n*_cache/\n_site/\nPush these changes, merge into main.\nOn GitHub.com, in your repo, set up GitHub publishing\nFollow instructions from the explore and setup chapter."
  },
  {
    "objectID": "transition-from-rmarkdown.html#troubleshooting",
    "href": "transition-from-rmarkdown.html#troubleshooting",
    "title": "Transition from RMarkdown",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nGitHub Action fails, says you need RMarkdown but you don’t have R code!\nAnd you changed all .Rmds to .qmds!\nYou likely have a few setup code chunks from RMarkdown, that look like this:\n{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\nYou can find them by opening each of your files and having a look, or use GitHub’s search for the keyword knitr"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zarr Visualization Cookbook",
    "section": "",
    "text": "This site documents different approaches and benchmarks for zarr visualization.\nThe intention is to support zarr data providers with some methods for visualizing zarr. This guide and report is intended to inform zarr data producers who want to understand the requirements for data pre-processing and chunking in order to support visualization through tiling server and direct client approahces.\n\nBackground\nVisualization of Earth science data is key to exploring and understanding Earth data. Web browsers offer a near-universal platform for exploring this data. However browsers of the web expect near instantaneous page rendering. The scale of geospatial data makes it challenging to serve this data “on-demand” as browsers cannot reproject and create image tiles fast enough for a good user experience.\nThe scale of geospatial data makes it challenging to serve this data “on-demand” as browsers cannot reproject and create image tiles fast enough for a good user experience. This challenge led to the development of pre-generated static map tiles.\nWhile pregenerated map tiles make it possible to visualize data quickly, there are drawbacks. The most significant is the data provider chooses how the data will appear. Next generation approaches give that power to the user. Other drawbacks impact the data provider, such as storage costs and maintaining a pipeline to constantly update or reprocess the tile storage with new and updated data. But the user is impacted by having no power to adjust the visualization, such as modifying the color scale, color map or perform “band math” where multiple variables are combined to produce a new variable.\nMore recent years have seen the success of the dynamic tiling approach which allows for on-demand map tile creation. This approach has traditionally relied on reading data from Cloud-Optimized GeoTIFFs (COGs). When the Zarr data format gained popularity for large-scale n-dimensional data analysis, users started to opine for browser-based visualization. The conventional Zarr chunk size stored for analysis (~100mb) was acknowledged to be too large to be fetched by a browser.\nNow there are 2 options: a dynamic tile server and a direct client. rio_tiler’s XarrayReader supports tile rendering from anything that is xarray-readable. This means a tile server can render tiles from Zarr stores as well as netCDF4/HDF5 and other formats. However, a tile server still requires running a server while the second option, a “direct client”, reads Zarr directly in the browser client and uses webGL to render map tiles.\nThis cookbook will describe these 2 approaches. We will discuss the tradeoffs, requirements for preprocessing the data and present performance testing results for when those preprocessing steps were taken or not. We hope that readers will be able to reuse lessons learned and recommendations to deliver their Zarr data to users in web browser and contribute to the wider adoption of this format for large scale environmental data understanding."
  },
  {
    "objectID": "approaches/tiling/recommendations.html",
    "href": "approaches/tiling/recommendations.html",
    "title": "Recommendations",
    "section": "",
    "text": "Recommendations\nCOMING SOON."
  },
  {
    "objectID": "approaches/tiling/cmip6-zarr-benchmarks.html",
    "href": "approaches/tiling/cmip6-zarr-benchmarks.html",
    "title": "CMIP6 Zarr Benchmarks",
    "section": "",
    "text": "Benchmarks were generated for multiple copies of the CMIP6 daily data to understand the performance for different data pre-processing options. These copies were differentiated by:\n\ndata file format (netCDF or Zarr), and,\ndifferent chunking configurations\n\nDetails on each dataset configuration:\n\nkerchunk + netCDF: A kerchunk reference file for NetCDF files stored on S3.\nZarr stores with different chunking configurations and pyramids.\n\nChunked to optimize for time series analysis:\n\nlatitude: 252, longitude: 252, time: 365.\nThis dataset has larger chunks, but more timesteps are loaded into each chunk.\n\nChunked to optimize for visualization at a single time step.\n\nlatitude: 600, longitude: 1440, time: 1.\nThis dataset has small chunks, but will likely not work well for time series generation.\n\nChunked to optimize for both time series and visualization:\n\nlatitude: 600, longitude: 1440, time: 29.\nThis dataset has larger chunks, but more timesteps are loaded into each chunk.\n\n\nZarr store with no coordinate chunking. At this time there is a known issue with pangeo-forge data generation where coordinates are chunked. This makes a significant impact on performance."
  },
  {
    "objectID": "approaches/tiling/cmip6-zarr-benchmarks.html#dataset-preprocessing",
    "href": "approaches/tiling/cmip6-zarr-benchmarks.html#dataset-preprocessing",
    "title": "CMIP6 Zarr Benchmarks",
    "section": "",
    "text": "Benchmarks were generated for multiple copies of the CMIP6 daily data to understand the performance for different data pre-processing options. These copies were differentiated by:\n\ndata file format (netCDF or Zarr), and,\ndifferent chunking configurations\n\nDetails on each dataset configuration:\n\nkerchunk + netCDF: A kerchunk reference file for NetCDF files stored on S3.\nZarr stores with different chunking configurations and pyramids.\n\nChunked to optimize for time series analysis:\n\nlatitude: 252, longitude: 252, time: 365.\nThis dataset has larger chunks, but more timesteps are loaded into each chunk.\n\nChunked to optimize for visualization at a single time step.\n\nlatitude: 600, longitude: 1440, time: 1.\nThis dataset has small chunks, but will likely not work well for time series generation.\n\nChunked to optimize for both time series and visualization:\n\nlatitude: 600, longitude: 1440, time: 29.\nThis dataset has larger chunks, but more timesteps are loaded into each chunk.\n\n\nZarr store with no coordinate chunking. At this time there is a known issue with pangeo-forge data generation where coordinates are chunked. This makes a significant impact on performance."
  },
  {
    "objectID": "approaches/tiling/cmip6-zarr-benchmarks.html#code-profiling-methodology",
    "href": "approaches/tiling/cmip6-zarr-benchmarks.html#code-profiling-methodology",
    "title": "CMIP6 Zarr Benchmarks",
    "section": "Code Profiling Methodology",
    "text": "Code Profiling Methodology\nThe time to generate a tile at zoom 0 to 11 was tested for each data store 10 times. The mean of these times is reported. Tests were run on the VEDA JupyterHub and the details can be reviewed in the profile.ipynb notebook of the tile-benchmarking repo.\nThe libraries used to generate image tiles are xarray for reading the Zarr metadata and rio_tiler’s XarrayReader for reading data from the NetCDFs on S3.\nCode from these libraries is copied into tile-benchmarking to generate tiles and is replicated in the titiler-xarray API codebase. Maintaining a copy enabled full control to add timers to blocks of code and logs to understand where time was being spent. Specifically:\n\nimport s3fs; s3fs.core.setup_logging(\"DEBUG\") was used to debug calls to S3. This was used to understand that the most time is spent opening the dataset, which was impacted by open all the coordinate chunks.\nTiming code blocks also demonstrated that the most time, other than opening the dataset, was spent in reprojecting the data. Time to reproject the data is positively correlated with the chunk size, since the minimum amount of data that can be read from S3 is the size of the data chunk."
  },
  {
    "objectID": "approaches/tiling/future-areas.html",
    "href": "approaches/tiling/future-areas.html",
    "title": "Future Areas",
    "section": "",
    "text": "Future Areas\nCOMING SOON."
  },
  {
    "objectID": "approaches/tiling/e2e-results.html",
    "href": "approaches/tiling/e2e-results.html",
    "title": "End to End Test Results",
    "section": "",
    "text": "This notebook parses results from end-to-end testing as documented in tile-benchmarking/e2e/e2e.ipynb."
  },
  {
    "objectID": "approaches/tiling/e2e-results.html#import-necessary-libraries",
    "href": "approaches/tiling/e2e-results.html#import-necessary-libraries",
    "title": "End to End Test Results",
    "section": "Import necessary libraries",
    "text": "Import necessary libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "approaches/tiling/e2e-results.html#define-the-data-location",
    "href": "approaches/tiling/e2e-results.html#define-the-data-location",
    "title": "End to End Test Results",
    "section": "Define the data location",
    "text": "Define the data location\n\ngit_url_path = \"https://raw.githubusercontent.com/developmentseed/tile-benchmarking/main/e2e\""
  },
  {
    "objectID": "approaches/tiling/e2e-results.html#define-some-helper-functions",
    "href": "approaches/tiling/e2e-results.html#define-some-helper-functions",
    "title": "End to End Test Results",
    "section": "Define some helper functions",
    "text": "Define some helper functions\n\ndef generate_zooms_plot(df, dataset):\n    zooms = [int(path.split('/')[2]) for path in df['Name'][:-1]]\n    # Create a new figure\n    plt.figure()\n\n    # Create a scatter plot\n    plt.scatter(zooms, df['Median Response Time'][:-1])\n\n    # Add title and labels\n    plt.title(dataset)\n    plt.xlabel('Zoom')\n    plt.ylabel('Time (ms)')\n\n    # Display the plot\n    plt.show() \n    \ndef git_url(filename):\n    return f'{git_url_path}/{filename}'\n\ndef add_to_specs(collection_name: str, df: pd.DataFrame):\n    zarr_specs.loc[collection_name, 'mean median response time'] = np.mean(df['Median Response Time'][1:])\n    zarr_specs.loc[collection_name, 'median median response time'] = np.median(df['Median Response Time'][1:])"
  },
  {
    "objectID": "approaches/tiling/e2e-results.html#parse-collection-results-into-a-data-frame",
    "href": "approaches/tiling/e2e-results.html#parse-collection-results-into-a-data-frame",
    "title": "End to End Test Results",
    "section": "Parse collection results into a data frame",
    "text": "Parse collection results into a data frame\nWhich also has the specs of the data, such as chunk size.\n\nzarr_specs = pd.read_csv(f\"{git_url_path}/zarr_info.csv\")\nzarr_specs.index = zarr_specs['collection_name']\n\nfor collection_name in zarr_specs.index:\n    filename = f'results/{collection_name}_urls_stats.csv'\n    results_df = pd.read_csv(git_url(filename))\n    add_to_specs(collection_name, results_df)"
  },
  {
    "objectID": "approaches/direct-client/benchmarking-methodology.html",
    "href": "approaches/direct-client/benchmarking-methodology.html",
    "title": "Benchmarking Methodology",
    "section": "",
    "text": "The end-to-end benchmarks capture the user experience in response to different actions. The suite of benchmarks included in this cookbook are designed to inform how the choice of Zarr versions and chunking schemes influence the user experience.\n\n\n\n\n\nCarbonPlan’s benchmark-maps repository leverages Playwright for the end-to-end performance benchmarks. By default, the benchmarks are run on https://prototype-maps.vercel.app/ although the url is configurable. The frame below shows this domain after selecting an approach, Zarr version, and dataset.\nThe benchmarking script takes the following steps:\n\nLaunch chromium browser\nCreate a new page\nStart chromium tracing\nSelect Zarr version in the dropdown\nSelect Dataset in the dropbown\nWait 5 seconds for the page the render\nZoom in a defined number of times, waiting 5 seconds after each action\nWrite out metadata about each run and the trace record\n\nThe frame rate and request information are extracted from the resultant metadata and trace records. The completion time for each zoom level is determined by comparing the screen captures in the trace record to the expected result for each zoom level."
  },
  {
    "objectID": "approaches/direct-client/benchmarking-methodology.html#end-to-end-benchmarks",
    "href": "approaches/direct-client/benchmarking-methodology.html#end-to-end-benchmarks",
    "title": "Benchmarking Methodology",
    "section": "",
    "text": "The end-to-end benchmarks capture the user experience in response to different actions. The suite of benchmarks included in this cookbook are designed to inform how the choice of Zarr versions and chunking schemes influence the user experience.\n\n\n\n\n\nCarbonPlan’s benchmark-maps repository leverages Playwright for the end-to-end performance benchmarks. By default, the benchmarks are run on https://prototype-maps.vercel.app/ although the url is configurable. The frame below shows this domain after selecting an approach, Zarr version, and dataset.\nThe benchmarking script takes the following steps:\n\nLaunch chromium browser\nCreate a new page\nStart chromium tracing\nSelect Zarr version in the dropdown\nSelect Dataset in the dropbown\nWait 5 seconds for the page the render\nZoom in a defined number of times, waiting 5 seconds after each action\nWrite out metadata about each run and the trace record\n\nThe frame rate and request information are extracted from the resultant metadata and trace records. The completion time for each zoom level is determined by comparing the screen captures in the trace record to the expected result for each zoom level."
  },
  {
    "objectID": "approaches/direct-client/recommendations.html",
    "href": "approaches/direct-client/recommendations.html",
    "title": "Recommendations",
    "section": "",
    "text": "This document details the preprocessing steps to deliver performant Zarr visualization via the direct client approach."
  },
  {
    "objectID": "approaches/direct-client.html",
    "href": "approaches/direct-client.html",
    "title": "Direct Client",
    "section": "",
    "text": "Traditional approaches to rendering raster datasets in the browser involve the creation of tiles in a pixelated image format like PNG or JPEG. These tiles can be pre-generated or created by a tiling server on demand, as described in the tiling approach chapter. While the image tiles are fast to load and easy to render, tiling offers limited flexibility for dynamically customizing visualizations based on user input. In contrast, the direct client approach leverages Zarr to render the data directly using WebGL rather than through an intermediate layer. The Zarr format is ideal for direct rendering in the browser because the chunks of a Zarr dataset serve a similar purpose to the tiles of a web map. In addition, data can be chunked across non-spatial dimensions like time, which removes the requirement of generating individual tiles per time step. Lastly, as a cloud-optimized data format Zarr allows for fast, parallel reading and writing from object storage.\nThe direct client approach leverages pyramids created with the ndpyramid package in order to performantly render data at multiple zoom levels. The approach loads Zarr data using the zarr-js JavaScript library and renders the fetched chunks via WebGL using the regl library. The open-source library called @carbonplan/maps provides a small set of React components for loading and rendering Zarr data using this approach and supports rendering traditional vector layers side-by-side using mapbox-gl-js.\nCarbonPlan has used this approach to quickly develop visualizations to showcase climate (some example visualizations)."
  },
  {
    "objectID": "approaches/direct-client.html#references",
    "href": "approaches/direct-client.html#references",
    "title": "Direct Client",
    "section": "References",
    "text": "References\nFreeman, J., K. Martin, and J. Hamman, 2021: A new toolkit for data-driven maps, https://carbonplan.org/blog/maps-library-release"
  }
]