[
  {
    "objectID": "approaches/tiling.html",
    "href": "approaches/tiling.html",
    "title": "Tiling",
    "section": "",
    "text": "The tiling approach is to provide image tiles via the XYZ Protocol and OGC API - Tiles API specifications.\nThis approach relies on the rio_tiler.XarrayReader library which includes the tile function. This function and others in that module are used to provide an API for tiles. An example API infrastructure can be found in titiler-xarray. Please note this library is still in development and is not intended for production use at this time."
  },
  {
    "objectID": "approaches/index.html",
    "href": "approaches/index.html",
    "title": "Approaches",
    "section": "",
    "text": "For browser-based visualization of Zarr, there are 2 approaches covered in this cookbook:\n\nTiling\nDirect Client\n\nThe tile server provides an API which is interoperable with multiple interfaces, but requires maintaining a tile server. also the response delivered to the client is an image format, not the raw data itself. The direct client has access to the underlying data and thus maximum flexibility in rendering and analysis for the user."
  },
  {
    "objectID": "approaches/direct-client/future-areas.html",
    "href": "approaches/direct-client/future-areas.html",
    "title": "Future Areas",
    "section": "",
    "text": "Future Areas"
  },
  {
    "objectID": "approaches/tiling/e2e.html",
    "href": "approaches/tiling/e2e.html",
    "title": "End-to-End Tests",
    "section": "",
    "text": "End-to-End Tests\nFrom https://github.com/developmentseed/tile-benchmarking/tree/main/e2e/README.md"
  },
  {
    "objectID": "approaches/tiling/future-areas.html",
    "href": "approaches/tiling/future-areas.html",
    "title": "Future Areas",
    "section": "",
    "text": "Future Areas"
  },
  {
    "objectID": "approaches/tiling/performance-methodology.html",
    "href": "approaches/tiling/performance-methodology.html",
    "title": "Performance Methodology",
    "section": "",
    "text": "Reproducibility is important to the integrity of this project and its reported results. That is why we selected a publicly available dataset and attempt to make the steps as fully documented and reproducible as possible. We used the NASA Earth Exchange Global Daily Downscaled Projections (NEX-GDDP-CMIP6) AWS public dataset for this project.\nThere are 2 datasets listed on AWS: 1 is an archive of NetCDF files from about 35 different climate models, each supplying historical and predicted values for up to 9 environment variables, daily, from 1950 to 2100. To minimize preprocessing, test datasets were generated for the first 2 years of historical data, for 1 model and for 1 variable. These variables or dataset could easily be modified or swapped, but we expect the relative performance using different datasets to be the same.\nIn addition to the NetCDF data, there is an archives of COGs generated from that NetCDF data to support visualization via dynamic tiling using COGs. COGs are only available for 2 models, so for inter comparison of the tiling approach between COGs and Zarr, one of those models (GISS-E2-1-G) is used to generate Zarr stores.\nAt this time, a different model is used for the direct client approach (ACCESS-CM2), but we will demonstrate how there is no meaningful difference in the performance of tiling across these models.\nBenchmarks for the tiling approach were run on the VEDA JupyterHub. The VEDA documentation details how to request access: https://nasa-impact.github.io/veda-docs/services/jupyterhub.html. Because we don’t want to make the database or S3 bucket with datasets fully public, you must be logged into the VEDA JupyterHub to run those benchmarks."
  },
  {
    "objectID": "approaches/tiling/performance-methodology.html#datasets",
    "href": "approaches/tiling/performance-methodology.html#datasets",
    "title": "Performance Methodology",
    "section": "",
    "text": "Reproducibility is important to the integrity of this project and its reported results. That is why we selected a publicly available dataset and attempt to make the steps as fully documented and reproducible as possible. We used the NASA Earth Exchange Global Daily Downscaled Projections (NEX-GDDP-CMIP6) AWS public dataset for this project.\nThere are 2 datasets listed on AWS: 1 is an archive of NetCDF files from about 35 different climate models, each supplying historical and predicted values for up to 9 environment variables, daily, from 1950 to 2100. To minimize preprocessing, test datasets were generated for the first 2 years of historical data, for 1 model and for 1 variable. These variables or dataset could easily be modified or swapped, but we expect the relative performance using different datasets to be the same.\nIn addition to the NetCDF data, there is an archives of COGs generated from that NetCDF data to support visualization via dynamic tiling using COGs. COGs are only available for 2 models, so for inter comparison of the tiling approach between COGs and Zarr, one of those models (GISS-E2-1-G) is used to generate Zarr stores.\nAt this time, a different model is used for the direct client approach (ACCESS-CM2), but we will demonstrate how there is no meaningful difference in the performance of tiling across these models.\nBenchmarks for the tiling approach were run on the VEDA JupyterHub. The VEDA documentation details how to request access: https://nasa-impact.github.io/veda-docs/services/jupyterhub.html. Because we don’t want to make the database or S3 bucket with datasets fully public, you must be logged into the VEDA JupyterHub to run those benchmarks."
  },
  {
    "objectID": "approaches/tiling/recommendations.html",
    "href": "approaches/tiling/recommendations.html",
    "title": "Recommendations",
    "section": "",
    "text": "Recommendations\nTBD"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zarr Visualization Cookbook",
    "section": "",
    "text": "This site documents different approaches and benchmarks for zarr visualization.\nThe intention is to support zarr data providers with some methods for visualizing zarr. This guide and report is intended to inform zarr data producers who want to understand the requirements for data pre-processing and chunking in order to support visualization through tiling server and direct client approahces.\n\nBackground\nVisualization of Earth science data is key to exploring and understanding Earth data. Web browsers offer a near-universal platform for exploring this data. However browsers of the web expect near instantaneous page rendering. The scale of geospatial data makes it challenging to serve this data “on-demand” as browsers cannot reproject and create image tiles fast enough for a good user experience.\nThe scale of geospatial data makes it challenging to serve this data “on-demand” as browsers cannot reproject and create image tiles fast enough for a good user experience. This challenge led to the development of pre-generated static map tiles.\nWhile pregenerated map tiles make it possible to visualize data quickly, there are drawbacks. The most significant is the data provider chooses how the data will appear. Next generation approaches give that power to the user. Other drawbacks impact the data provider, such as storage costs and maintaining a pipeline to constantly update or reprocess the tile storage with new and updated data. But the user is impacted by having no power to adjust the visualization, such as modifying the color scale, color map or perform “band math” where multiple variables are combined to produce a new variable.\nMore recent years have seen the success of the dynamic tiling approach which allows for on-demand map tile creation. This approach has traditionally relied on reading data from Cloud-Optimized GeoTIFFs (COGs). When the Zarr data format gained popularity for large-scale n-dimensional data analysis, users started to opine for browser-based visualization. The conventional Zarr chunk size stored for analysis (~100mb) was acknowledged to be too large to be fetched by a browser.\nNow there are 2 options: a dynamic tile server and a direct client. rio_tiler’s XarrayReader supports tile rendering from anything that is xarray-readable. This means a tile server can render tiles from Zarr stores as well as netCDF4/HDF5 and other formats. However, a tile server still requires running a server while the second option, a “direct client”, reads Zarr directly in the browser client and uses webGL to render map tiles.\nThis cookbook will describe these 2 approaches. We will discuss the tradeoffs, requirements for preprocessing the data and present performance testing results for when those preprocessing steps were taken or not. We hope that readers will be able to reuse lessons learned and recommendations to deliver their Zarr data to users in web browser and contribute to the wider adoption of this format for large scale environmental data understanding."
  },
  {
    "objectID": "transition-from-rmarkdown.html",
    "href": "transition-from-rmarkdown.html",
    "title": "Transition from RMarkdown",
    "section": "",
    "text": "You may already have workflows in RMarkdown and are interested in transitioning to Quarto. There’s no hurry to migrate to Quarto. Keep using Rmarkdown and when you’re ready the migration will be fine.\nHere are some notes as we migrate RMarkdown sites and books.\nTODO: translating R code chunks"
  },
  {
    "objectID": "transition-from-rmarkdown.html#bookdown-to-quarto",
    "href": "transition-from-rmarkdown.html#bookdown-to-quarto",
    "title": "Transition from RMarkdown",
    "section": "Bookdown to Quarto",
    "text": "Bookdown to Quarto\nConverting a Bookdown book to Quarto is slightly more involved than converting a website. A book has chapters whose order must be defined, and likely has citations and cross-refs. Still, conversion is not that hard.\nWe got some practice converting from Bookdown to Quarto by helping Gavin Fay convert his lab’s fantastic onboarding documentation, the Faylab Lab Manual. Here’s the GitHub view before and after.\nOur best first reference material for this was Nick Tierney’s Notes on Changing from Rmarkdown/Bookdown to Quarto. Nick shares some scripts in that post to automate some changes. In our case, the book was small enough that we made all changes manually. Quarto documentation was indispensable.\n\nExperimenting in a low-risk environment\nWe forked a copy of the Faylab Lab manual to the Openscapes organization, and worked in a branch so we could make changes relatively risk-free. We could always fork a new copy of the original if we “broke” something. (Caution: the default when making a pull request from a fork is to push changes to the original upstream repo, not your fork and it does this without warning if you have write-access to that repo.) With local previews it’s easy to test / play with settings to see what they do. We tended to make a change, Preview, then compare the look and functionality of the book to the original. It was helpful to comment out some elements of the configuration file _output.yml after their counterparts had been added to the Quarto configuration file _quarto.yml, or to confirm they were no longer needed, before making the drastic move of deleting them.\n\n\nThe conversion\nHere are the main steps to convert the Faylab Lab manual from Bookdown to Quarto.\nCreate new empty file called _quarto.yml and add book metadata there. The screenshots below\nSet project type as book.\nMove metadata out of index.qmd and into _quarto.yml. Title, author, and publication date were in index.qmd with date set using date: \"Last updated:r Sys.Date()\". Now these are in _quarto.yml with date set using date: last-modified. Note that having R code would require you to adjust code chunk options in the Quarto style (#|). This tripped us up a bit; see GitHub Actions.\nMove chapters listing out of _bookdown.yml and into _quarto.yml.\nAdd page footer to _quarto.yml.\nHere’s what ours looked like when we finished the steps above (_quarto.yml).\n\n\n\n\n\n\n_quarto.yml contents\n\n\n\n\n\n\n\nFaylab Lab Manual\n\n\n\n\n\nChange insertion of images from html style to Quarto style. (Note Quarto calls them “figures”, not “images”.) The following snippet will insert the GitHub octocat logo in a page:\n![](https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png){fig-align=\"left\" width=\"35px\"}\nChange all filename extensions .Rmd -&gt; .qmd (you could Preview after this change and see that the book looks the same). Note that Quarto works with .Rmd files just as well as it does .qmd, so this change is not urgent. In fact, if you have a lot of R code in your .Rmds (unlike the Faylab Lab Manual), there will be additional tinkering needed to make the code chunks happy.\n\n\nCitations\nThe Faylab Lab Manual cited two papers, presenting us with an opportunity to see how easy it is to add references to a Quarto book. Briefly, in the Visual Editor, Insert &gt; Citation &gt; DOI. Pasting the DOI or its full URL, we can insert the citation. This automatically creates a references.bib file and adds the full citations at the bottom of the chapter page (watch demo). In July 2022, we had to manually add a ## References heading, but this may not be necessary in future Quarto updates.\n\n\n\n\n\n\nInsert citation via its DOI using RStudio Visual Editor\n\n\n\n\n\n\n\n\n\n\nPublishing notes\nIf the book’s output is strictly html, there’s no need to specify output-dir in _quarto.yml. The output directory default is _book/, which is what we’d like. If we wanted other types of output like like PDF or EPUB, etc. those single file outputs are also written to the output-dir (Quarto docs).\nIf you currently have a docs/ folder, delete it.\nUpdate .gitignore to ignore _book/. At the same time, we have it ignore caches and a .quarto file:\n/.quarto/\n*_cache/\n_book/\nOnce all is settled, delete _output.yml.\nOnce the Openscapes fork was fully reviewed, we made a pull request from that to the main branch of the book’s repo. Once that was merged, we set up GitHub Actions to render the book. (TODO: instructions for GitHub Actions)\n\n\nGitHub Actions\nThis book was mostly prose and screenshots without any R code. This made the conversion from RMarkdown to Quarto likely more straightforward than if you also needed to adjust code chunk options in the quarto style (#|). Our initial GitHub Action to render the converted Faylab Lab Manual failed because we had a piece of R code - even though the code was commented out! This was resolved when we deleted the line."
  },
  {
    "objectID": "transition-from-rmarkdown.html#distill-to-quarto",
    "href": "transition-from-rmarkdown.html#distill-to-quarto",
    "title": "Transition from RMarkdown",
    "section": "Distill to quarto",
    "text": "Distill to quarto\nWe transitioned our events site from distill to quarto in May 2022 (github view before and after). We followed excellent notes and examples from Nick Tierney and Danielle Navarro.\nAfter we had changed all the files, the Build tab in the RStudio IDE still showed “Build website” rather then “Render Website” and “Preview Website”, and would error when we pushed them (because that button was expecting a distill site, not a quarto site). To fix this, we updated the .Rproj file. Clicking on the .Rproj file in the RStudio IDE will open a dialog box where you can click things you want (you can also open these in a text editor or from the GitHub website to see the actual text). To fix this situation with the Build tab: Project Options &gt; Build Tools &gt; Project Build Tools &gt; None.\nLooking at files /posts/_metadata.yml and _quarto.yml helps see where things are defined. For example, to make event post citations appear, we added citation: true to /posts/_metadata.yml and in _quarto.yml under the website key we set site-url: https://openscapes.github.io/events. We deleted footer.html used with distill because footer is now defined in quarto.yml.\n\nPublishing notes\n\nBackground: Our distill site had been set up to output to a docs folder, and had GitHub Settings &gt; Pages set to look there rather gh-pages branch. (Julie note: this was a new-to-me capability when we set up the events distill site in Spring 2021 so I had forgotten that was an option). We’ve inititally kept this same set-up for now with our events page in _quarto.yml: output-dir: docs. However, this is sub-optimal; better to not have to commit and push these files but to instead have a GitHub Action generate them upon a commit. So the following is what we did -\n\nDon’t specify output-dir in _quarto.yml. The output directory default is _site/, which is what we’d like.\nIf you currently have a docs/ folder (like we did as we were experimenting), delete it.\nUpdate .gitignore to ignore _site/. At the same time, we have it ignore caches and a .quarto file:\n/.quarto/\n*_cache/\n_site/\nPush these changes, merge into main.\nOn GitHub.com, in your repo, set up GitHub publishing\nFollow instructions from the explore and setup chapter."
  },
  {
    "objectID": "transition-from-rmarkdown.html#troubleshooting",
    "href": "transition-from-rmarkdown.html#troubleshooting",
    "title": "Transition from RMarkdown",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nGitHub Action fails, says you need RMarkdown but you don’t have R code!\nAnd you changed all .Rmds to .qmds!\nYou likely have a few setup code chunks from RMarkdown, that look like this:\n{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\nYou can find them by opening each of your files and having a look, or use GitHub’s search for the keyword knitr"
  },
  {
    "objectID": "approaches/tiling/results-summary.html",
    "href": "approaches/tiling/results-summary.html",
    "title": "Results Summary",
    "section": "",
    "text": "Results Summary\nFrom https://github.com/developmentseed/tile-benchmarking/blob/main/profiling/profile.ipynb and https://github.com/developmentseed/tile-benchmarking/blob/main/e2e/read-results.ipynb"
  },
  {
    "objectID": "approaches/tiling/pgstac-cog.html",
    "href": "approaches/tiling/pgstac-cog.html",
    "title": "pgSTAC + COG results",
    "section": "",
    "text": "pgSTAC + COG results\nFrom https://github.com/developmentseed/tile-benchmarking/blob/main/profiling/profile.ipynb"
  },
  {
    "objectID": "approaches/tiling/zarr.html",
    "href": "approaches/tiling/zarr.html",
    "title": "Zarr Test Results",
    "section": "",
    "text": "Zarr Test Results\nFrom https://github.com/developmentseed/tile-benchmarking/blob/main/profiling/profile.ipynb"
  },
  {
    "objectID": "approaches/direct-client/preprocessing.html",
    "href": "approaches/direct-client/preprocessing.html",
    "title": "Preprocessing",
    "section": "",
    "text": "Preprocessing\nThis document details the preprocessing steps to deliver performant Zarr visualization via the direct client approach."
  },
  {
    "objectID": "approaches/direct-client/benchmarks.html",
    "href": "approaches/direct-client/benchmarks.html",
    "title": "Benchmarks",
    "section": "",
    "text": "Benchmarks\nTo be merged with https://github.com/carbonplan/benchmark-maps"
  },
  {
    "objectID": "approaches/direct-client.html",
    "href": "approaches/direct-client.html",
    "title": "Direct Client",
    "section": "",
    "text": "Traditional approaches to rendering raster datasets in the browser involve the creation of tiles in a pixelated impact format like PNG or JPEG. These tiles can be pre-generated or created by a tiling server on demand, as described in the tiling approach chapter. While the image tiles are fast to load and easy to render, tiling offers limited flexibility for dynamically customizing visualizations based on user input. In contrast, the direct client approach leverages Zarr to render the data directly using WebGL rather than through an intermediate layer. The Zarr format is ideal for direct rendering in the browser because the chunks of a Zarr dataset serve a similar purpose to the tiles of a web map. In additional, data can be chunked across additional dimensions which prevents the need for generating individual tiles per time step. Lastly, as a cloud-optimized data format Zarr allows for fast, parallel reading and writing from object storage.\nThe direct client approach leverages pyramids created with the ndpyramid package for performant rendering of data at multiple zoom levels. The approach loads Zarr V2 or V3 data using the zarr-js JavaScript library and renders the fetched chunks via WebGL using the regl library. The open-source library called @carbonplan/maps provides a small set of React components for rendering raster data using this approach and supports rendering traditional vector layers side-by-side using mapbox-gl-js.\n\n\n\nFreeman, J., K. Martin, and J. Hamman, 2021: A new toolkit for data-driven maps, https://carbonplan.org/blog/maps-library-release"
  },
  {
    "objectID": "approaches/direct-client.html#introduction",
    "href": "approaches/direct-client.html#introduction",
    "title": "Direct Client",
    "section": "",
    "text": "Traditional approaches to rendering raster datasets in the browser involve the creation of tiles in a pixelated impact format like PNG or JPEG. These tiles can be pre-generated or created by a tiling server on demand, as described in the tiling approach chapter. While the image tiles are fast to load and easy to render, tiling offers limited flexibility for dynamically customizing visualizations based on user input. In contrast, the direct client approach leverages Zarr to render the data directly using WebGL rather than through an intermediate layer. The Zarr format is ideal for direct rendering in the browser because the chunks of a Zarr dataset serve a similar purpose to the tiles of a web map. In additional, data can be chunked across additional dimensions which prevents the need for generating individual tiles per time step. Lastly, as a cloud-optimized data format Zarr allows for fast, parallel reading and writing from object storage.\nThe direct client approach leverages pyramids created with the ndpyramid package for performant rendering of data at multiple zoom levels. The approach loads Zarr V2 or V3 data using the zarr-js JavaScript library and renders the fetched chunks via WebGL using the regl library. The open-source library called @carbonplan/maps provides a small set of React components for rendering raster data using this approach and supports rendering traditional vector layers side-by-side using mapbox-gl-js."
  },
  {
    "objectID": "approaches/direct-client.html#references",
    "href": "approaches/direct-client.html#references",
    "title": "Direct Client",
    "section": "",
    "text": "Freeman, J., K. Martin, and J. Hamman, 2021: A new toolkit for data-driven maps, https://carbonplan.org/blog/maps-library-release"
  }
]